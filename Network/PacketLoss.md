### What is Packet Loss ?
Packet loss occurs when one or more packets of data travelling across a computer network fail to reach their destination. Packet loss is typically caused by network congestion. Packet loss is measured as a percentage of packets lost with respect to packets sent.
The Transmission Control Protocol (TCP) detects packet loss and performs retransmissions to ensure reliable messaging. Packet loss in a TCP connection is also used to avoid congestion and reduces throughput of the connection.

### How can it happen?
Packet loss is typically caused by **network congestion**. When content arrives for a sustained period at a given router or network segment at a rate greater than it is possible to send through, then there is no other option than to drop packets. If a single router or link is constraining the capacity of the complete travel path or of network travel in general, it is known as a **bottleneck**.
Packet loss can be caused by a number of other factors that can corrupt or lose packets in transit, such as radio signals that are too weak due to distance or multi-path fading (in radio transmission), faulty networking hardware, or faulty network drivers. Packets are also intentionally dropped by normal routing routines (such as Dynamic Source Routing in ad hoc networks, ) and through network dissuasion technique for operational management purposes.
Packet loss can also be caused by a packet drop attack.

### Effect
Packet loss can reduce throughput for a given sender, whether unintentionally due to network malfunction, or intentionally as a means to balance available bandwidth between multiple senders when a given router or network link reaches nears its maximum capacity.
When reliable delivery is necessary, packet loss increases latency due to additional time needed for retransmission. Assuming no retransmission, packets experiencing the worst delays might be preferentially dropped (depending on the queuing discipline used) resulting in lower latency overall at the price of data loss.
During typical network congestion, not all packets in a stream are dropped. This means that undropped packets will arrive with low latency compared to retransmitted packets, which arrive with high latency. Not only do the retransmitted packets have to travel part of the way twice, but the sender will not realize the packet has been dropped until it either fails to receive acknowledgement of receipt in the expected order, or fails to receive acknowledgement for a long enough time that it assumes the packet has been dropped as opposed to merely delayed.

### Rationale
The Internet Protocol (IP) is designed according to the **end-to-end principle** as a **best-effort delivery** service, with the intention of keeping the logic routers must implement as simple as possible. If the network made **reliable delivery** guarantees on its own, that would require **store and forward** infrastructure, where each router devoted a significant amount of storage space to packets while it waited to verify that the next node properly received it. A reliable network would not be able to maintain its delivery guarantees in the event of a router failure. Reliability is also not needed for all applications. For example, with a live audio stream, it is more important to deliver recent packets quickly than to ensure that stale packets are eventually delivered. An application may also decide to retry an operation that is taking a long time, in which case another set of packets will be added to the burden of delivering the original set. Such a network might also need a command and control protocol for congestion management, adding even more complexity.
To avoid all of these problems, the Internet Protocol allows for routers to simply drop packets if the router or a network segment is too busy to deliver the data in a timely fashion, or if the IPv4 header checksum indicates the packet has been corrupted. Obviously this is not ideal for speedy and efficient transmission of data, and is not expected to happen in an uncongested network. Dropping of packets acts as an implicit signal that the network is congested, and may cause senders to reduce the amount of bandwidth consumed, or attempt to find another path. For example, the Transmission Control Protocol (TCP) is designed so that excessive packet loss will cause the sender to throttle back and stop flooding the bottleneck point with data (using perceived packet loss as feedback to discover congestion).

### Packet recovery for reliable delivery
**The Internet Protocol leaves responsibility for any retransmission of dropped packets, or "packet recovery" to the endpoints - the computers sending and receiving the data. They are in the best position to decide whether retransmission is necessary, because the application sending the data should know whether speed is more important than reliability, whether a message should be re-attempted in whole or in part, whether or not the need to send the message has passed, and how to vary the amount of bandwidth consumed to account for any congestion.**
Some network transport protocols such as TCP provide endpoints an easy way to ensure reliable delivery of packets, so that individual applications don't need to implement logic for this themselves. In the event of packet loss, the receiver asks for retransmission or the sender automatically resends any segments that have not been acknowledged.[11] Although TCP can recover from packet loss, retransmitting missing packets causes the throughput of the connection to decrease. This drop in throughput is due to the sliding window protocols used for acknowledgment of received packets. In certain variants of TCP, if a transmitted packet is lost, it will be re-sent along with every packet that had been sent after it. This retransmission causes the overall throughput of the connection to drop.
**Protocols such as User Datagram Protocol (UDP) provide no recovery for lost packets. Applications that use UDP are expected to define their own mechanisms for handling packet loss.**
